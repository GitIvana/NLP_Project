{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dIgF24fu9b7",
        "outputId": "451fc6a3-0da6-4eed-a874-21276f7dbc1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import os\n",
        "import json\n",
        "import tweepy\n",
        "from google.colab import drive\n",
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "JfZ8S046Anh0",
        "outputId": "0e6ddd6e-4cb6-4cb6-f23f-5a9054b4484b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tweepy==4.8.0\n",
            "  Downloading tweepy-4.8.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.8.0) (3.2.0)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.8.0) (1.3.1)\n",
            "Collecting requests<3,>=2.27.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (2021.10.8)\n",
            "Installing collected packages: requests, tweepy\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed requests-2.27.1 tweepy-4.8.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests",
                  "tweepy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tweepy==4.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_EiVwRJ2oJN",
        "outputId": "b673afa1-ed6d-4fd6-b498-e97d10fee1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import os\n",
        "import json\n",
        "import tweepy\n",
        "from google.colab import drive\n",
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# set tweet key and secrect\n",
        "consumer_keyNow = 'Bkzg44MHgtQszMvCdFhJEindf'\n",
        "consumer_key_secretNow = '07vhPzTsz2r6QTy3TzISuDN0NJZNLt6SMBJIcGpbIFEZunwsGw'\n",
        "access_tokenNow = '1512683317820239876-KphHJoncgY9fJpr5yw8WvEJdRYXZzF'\n",
        "access_token_secretNow = 'gnUcKp6V7KrkUTLJbeTs5BCkiinevc1jkt181djFhUVmW'\n",
        "bearer_tokenNow = 'AAAAAAAAAAAAAAAAAAAAAKMxbQEAAAAAXL9RA3fotZaAejPj5lYjvdm9DaE%3DQ7BpU1w6ZxBBzEiOaKqBXA1lIuUISRptYlxAC6SaOBOA9Drfw9'\n",
        "client = tweepy.Client(bearer_tokenNow, consumer_key= consumer_keyNow,consumer_secret= consumer_key_secretNow,access_token= access_token_secretNow,access_token_secret= access_tokenNow, wait_on_rate_limit= True)\n",
        "\n",
        "apiList = []\n",
        "apiDict = [ {\"consumer_key\": \"XFEcxTOgeBOqrm93oi8HaGOV0\", \"consumer_secret\": \"nuWBsAq35YwFQD3M0XhGaHQmazb0l97aTxYdPUb99fFWhdn9xv\", \"access_token_key\": \"1513721883568787456-PfsFmjlJabzIZzZ0I48oEEOrT2toDb\", \"access_token_secret\": \"1qfH3VCR9egkvQXtUvKs1qnu2a3ubRuvvvTS93nCJCmNI\", \"bearer_token\": \"AAAAAAAAAAAAAAAAAAAAACr1bwEAAAAA%2BLXtPE4xYA%2FkLERzJdvu%2BQ5SghY%3DtRw00y62xzJj6ktJGmsNcCvvbJBvdRp0Ln3v8cqHPAv8SGLGtL\"},{\"consumer_key\": \"PBWjRTxSaanQoI33STur380CS\", \"consumer_secret\": \"POPUnpMbOTs0m721mnfTQRUMZ14rXqcUaPcMIQjjhch6wAqz6l\", \"access_token_key\": \"1514215434110734336-RoXlXdIxUqCOpilAMXiIBxVAqwiBpT\", \"access_token_secret\": \"kDNYIteibDvq1u2oeXwBkL4jeBf9E8Kqxxb8YUtVDlfMk\", \"bearer_token\": \"AAAAAAAAAAAAAAAAAAAAAFbrbwEAAAAAC1wy50bsb8qQYA9nK37G1xh9LVE%3DJ6G6CclRKUSp7CFH1nLiRS7Ola3Xg9nxZiH4MlHDAIhEOIrKFm\"},{\"consumer_key\": \"B5d80GNgf0htlN6BQM6GyYATf\", \"consumer_secret\": \"qgpxFGhb6oRcT7DxJtXSGT7FCQrlpiapIirm0j4wjesRNbsoA6\", \"access_token_key\": \"1081497692281417729-7ImilqK4akdwyGVTsniKNMoPGI7PuC\", \"access_token_secret\": \"Iws2mN2cirOCLB6oNCQ7xc6rwjUUSvqLudI1uVN1Omtlv\", \"bearer_token\": \"AAAAAAAAAAAAAAAAAAAAANP1bwEAAAAAT5eE2A91XgW71d7szYKYaLMqvHo%3DZ2S2Y9p2fOKxBOn9IfhzfHXIdIYufgSXBhH6xpQAI7NYcAvMZF\"},{\"consumer_key\": \"97NZMdQiYkDd6obbuMWWa9xdn\", \"consumer_secret\": \"jrKgbE7JHqo7bVGZUUbqgIoYtRvOsMg2fmJCJYnZsyPakMgGl8\", \"access_token_key\": \"1519879315579895808-cMiBiMVmtlUISfuRroDsjZpHa8pWXn\", \"access_token_secret\": \"fWtilQislbFHVWD8doi1YqZIPkmHpW1KXg6HMnnCUOATu\", \"bearer_token\": \"AAAAAAAAAAAAAAAAAAAAAFIOcAEAAAAAd7pqY5AN04VUGhNUkuPmOCOGwGk%3DXdypxk16XugMPesVJCrkxLAHaEkGBb14Vzc99NIICfxHyn5fp3\"},{\"consumer_key\": \"CUAGDKipxKb5wKKr2WPPEOp0m\", \"consumer_secret\": \"Cls2eFnOsY5HuqUCdXONgK04cLW1e5pq7cffIK5lg7tI3w6HF7\", \"access_token_key\": \"1519804086337499136-Pqd3A4PgoQqwYVd6EiagDvsJRV0Uv0\", \"access_token_secret\": \"JYm51jkLcn14I3RGOBRm3O71aQFSQixQM22fKZBddg67s\", \"bearer_token\": \"AAAAAAAAAAAAAAAAAAAAALEOcAEAAAAAkIWSxwXYGbcU8fSaqzsxN3BNqK4%3DcvJKEe1CX9fPKLb3qwEXKR2UTq6uL1ggg8MGeQVyi6PUzR6ecw\"},{\"consumer_key\": \"vBiXbWeFeOPf7BPEuPKXSkB9r\", \"consumer_secret\": \"8yPcjeHvQlV1PXjd12XQyVZoEI6gT4VWOGr1Qi74wpCT73BE0o\", \"access_token_key\": \"1512683317820239876-pV82VUFemuKDCiWy6UHJq2vRIpgZa2\", \"access_token_secret\": \"5D8PYXkSNXIMRVslpyAQSkJpWOFXGnnXiHiljzodbtXFw\", \"bearer_token\": \"AAAAAAAAAAAAAAAAAAAAAKsMcAEAAAAAjJKmPixhdJBwv0k0gsJrtJddEww%3DibfJ3mdNeF3jsbQZTaKeln0pvH3VeoqkwsPD9MLUSyLcroNAl6\"}]\n",
        "# set tweet key and secrect\n",
        "for apiAccess in apiDict:\n",
        "  consumer_key = apiAccess['consumer_key']\n",
        "  consumer_key_secret= apiAccess['consumer_secret']\n",
        "  access_token = apiAccess['access_token_key']\n",
        "  access_token_secret = apiAccess['access_token_secret']\n",
        "  bearer_token = apiAccess['bearer_token']  \n",
        "  client = tweepy.Client(bearer_token, consumer_key= consumer_key,consumer_secret= consumer_key_secret,access_token= access_token,access_token_secret= access_token_secret, wait_on_rate_limit= True)\n",
        "  apiList.append(client)\n",
        "\n",
        "\n",
        "# load tweet ID by given file name\n",
        "def read_txt(file_name):\n",
        "    # load file\n",
        "    file = open(os.path.join(__location__, file_name),encoding='utf-8-sig')\n",
        "    source_reply = {}\n",
        "    index = 1\n",
        "    with io.open(os.path.join(__location__, file_name), 'r',encoding='utf-8') as lines:\n",
        "      for line in lines:\n",
        "        line = line[:-2]\n",
        "        sequence = line.split(',')\n",
        "        source_reply[index] = sequence\n",
        "        index +=1 \n",
        "    return source_reply\n",
        "\n",
        "def crawl_tweet_byID_byClient(ID, client):\n",
        "    tweetsResponse = client.get_tweets(\n",
        "        ids= ID,\n",
        "        expansions=[\"attachments.poll_ids\", \"attachments.media_keys\", \"author_id\", \"entities.mentions.username\", \"geo.place_id\", \"in_reply_to_user_id\", \"referenced_tweets.id\", \"referenced_tweets.id.author_id\"],\n",
        "        media_fields=[\"duration_ms\",\"height\",\"media_key\",\"preview_image_url\",\"type\",\"url\",\"width\",\"public_metrics\",\"non_public_metrics\",\"organic_metrics\",\"promoted_metrics\",\"alt_text\"],\n",
        "        place_fields = [\"contained_within\",\"country\",\"country_code\",\"full_name\",\"geo\",\"id\",\"name\",\"place_type\"],\n",
        "        poll_fields = [\"duration_minutes\", \"end_datetime\", \"id\", \"options\", \"voting_status\"],\n",
        "        tweet_fields = [\"attachments\", \"author_id\", \"context_annotations\", \"conversation_id\", \"created_at\", \"entities\", \"geo\", \"id\", \"in_reply_to_user_id\", \"lang\", \"public_metrics\", \"possibly_sensitive\", \"referenced_tweets\", \"reply_settings\", \"source\", \"text\", \"withheld\"],\n",
        "        user_fields = [\"created_at\", \"description\", \"entities\", \"id\", \"location\", \"name\", \"pinned_tweet_id\", \"profile_image_url\", \"protected\", \"public_metrics\", \"url\", \"username\", \"verified\", \"withheld\"],\n",
        "        user_auth=True\n",
        "    )\n",
        "    # tackle tweets as dict\n",
        "    tweetsList = []\n",
        "    for tweetResponse in tweetsResponse.data:\n",
        "      tweetDict = {}\n",
        "      for key in list(tweetResponse.keys()):\n",
        "        tweetDict[key] = tweetResponse[key]\n",
        "      tweetsList.append(tweetDict)\n",
        "\n",
        "    return tweetsList\n",
        "\n",
        "\n",
        "def crawl_tweets(idx, client):\n",
        "  # conjunt the user id as list\n",
        "  tweetsIDList = []\n",
        "  for i in range(len(idx.keys())):\n",
        "    tweetsIDList += idx[i+1]\n",
        "  # call tweet api\n",
        "  tweetsList = []\n",
        "  i = 0\n",
        "  while tweetsIDList != []:\n",
        "    tweetsIDSlot = []\n",
        "    while tweetsIDList != [] and len(tweetsIDSlot) < 100:\n",
        "      tweetsIDSlot.append(tweetsIDList.pop(0))\n",
        "    response = crawl_tweet_byID_byClient(tweetsIDSlot, client)\n",
        "    tweetsList += response\n",
        "    print(len(tweetsList), (i+1)*100)\n",
        "    i += 1\n",
        "\n",
        "  return tweetsList\n",
        "\n",
        "def crawl_tweets_clientList(idx, clientList):\n",
        "  # conjunt the user id as list\n",
        "  tweetsIDList = []\n",
        "  for i in range(len(idx.keys())):\n",
        "    tweetsIDList += idx[i+1]\n",
        "  print(len(tweetsIDList))\n",
        "  # call tweet api\n",
        "  tweetsList = []\n",
        "  i = 0\n",
        "  j = 0\n",
        "  while tweetsIDList != []:\n",
        "    tweetsIDSlot = []\n",
        "    while tweetsIDList != [] and len(tweetsIDSlot) < 100:\n",
        "      tweetsIDSlot.append(tweetsIDList.pop(0))\n",
        "    j += 1\n",
        "    try:\n",
        "      response = crawl_tweet_byID_byClient(tweetsIDSlot, clientList[j%len(clientList)])\n",
        "      tweetsList += response\n",
        "      print(len(tweetsList), (i+1)*100, (i+1) * 100 * 100 / len(tweetsIDList) + \"%\")\n",
        "      i += 1\n",
        "    except:\n",
        "      tweetsIDList = tweetsIDSlot + tweetsIDList\n",
        "\n",
        "  return tweetsList\n",
        "\n",
        "def conjunt_tweets_result(tweetsListFull,idx):\n",
        "  # get partition of timeline\n",
        "  tweetsIDList = [IDs for IDs in list(idx.values())]\n",
        "  # get id of each tweet\n",
        "  tweetsListFullKeys = [ str(tweet['id']) for tweet in tweetsListFull]\n",
        "\n",
        "  IDsDiv = []\n",
        "  for IDs in tweetsIDList:\n",
        "    IDsIndex = []\n",
        "    for ID in IDs:\n",
        "      try:\n",
        "        IDsIndex.append(tweetsListFull[tweetsListFullKeys.index(ID)])\n",
        "      except:\n",
        "        pass\n",
        "    IDsDiv.append(IDsIndex)\n",
        "\n",
        "  return IDsDiv\n",
        "\n",
        "def convertFormat(tweetsDivs):\n",
        "  for tweetsDiv in tweetsDivs:\n",
        "    for tweet in tweetsDiv:\n",
        "      dicReferences = {}\n",
        "      try:\n",
        "        for referenced_tweet in tweet['referenced_tweets']:\n",
        "          dicReferences[referenced_tweet['id']] = referenced_tweet['type']\n",
        "        tweet['referenced_tweets'] = dicReferences\n",
        "      except:\n",
        "        pass\n",
        "      try:\n",
        "        tweet['created_at'] = tweet['created_at'].strftime('%m/%d/%Y')\n",
        "      except:\n",
        "        pass\n",
        "  return tweetsDivs\n",
        "\n",
        "def cleanTweets(tweets):\n",
        "  res = []\n",
        "  for i in range(len(tweets)):\n",
        "    if tweets[i]['lang'] == 'en':\n",
        "      res.append(tweets[i])\n",
        "  return res\n",
        "\n",
        "def saveTweets(tweets, idx, location):\n",
        "  tweetsDiv = conjunt_tweets_result(tweets, idx)\n",
        "  tweetsDiv = convertFormat(tweetsDiv)\n",
        "  with open(location, 'w') as fout:\n",
        "    json.dump(tweetsDiv, fout)\n",
        "\n",
        "__location__ = '/content/drive/MyDrive/nlpKaggle/nlpTrainningData/project-data/'\n",
        "\n",
        "location_train = 'train_tweets/train_tweets.json'\n",
        "location_test = 'test_tweets/test_tweets.json'\n",
        "location_dev = 'dev_tweets/dev_tweets.json'\n",
        "location_covid = 'covid_tweets/covid_tweets.json'\n",
        "\n",
        "train_set_idx = read_txt('train.data.txt')\n",
        "test_set_idx = read_txt('test.data.txt')\n",
        "dev_set_idx = read_txt('dev.data.txt')\n",
        "covid_set_idx = read_txt('covid.data.txt')\n",
        "\n",
        "# # trainingTweets = crawl_tweets(train_set_idx, client)\n",
        "# trainingTweets = cleanTweets(trainingTweets)\n",
        "# saveTweets(trainingTweets, train_set_idx, __location__ + location_train)\n",
        "\n",
        "# # testingTweets = crawl_tweets(test_set_idx, client)\n",
        "# testingTweets = cleanTweets(testingTweets)\n",
        "# saveTweets(testingTweets, test_set_idx, __location__ + location_test)\n",
        "\n",
        "# # devTweets = crawl_tweets(dev_set_idx, client)\n",
        "# devTweets = cleanTweets(devTweets)\n",
        "# saveTweets(devTweets, dev_set_idx, __location__ + location_dev)\n",
        "\n",
        "# covidTweets = crawl_tweets(covid_set_idx, client)\n",
        "# covidTweets = cleanTweets(covidTweets)\n",
        "# saveTweets(covidTweets, covid_set_idx, __location__ + location_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwxLOsOVFi92"
      },
      "outputs": [],
      "source": [
        "auth = tweepy.OAuthHandler(\"vBiXbWeFeOPf7BPEuPKXSkB9r\", \"8yPcjeHvQlV1PXjd12XQyVZoEI6gT4VWOGr1Qi74wpCT73BE0o\")\n",
        "auth.set_access_token(\"1512683317820239876-pV82VUFemuKDCiWy6UHJq2vRIpgZa2\", \"5D8PYXkSNXIMRVslpyAQSkJpWOFXGnnXiHiljzodbtXFw\")\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "tweetCrawler = tweepy.Cursor(api.user_timeline, user_id = 31013444).items(1000)\n",
        "tweetList = []\n",
        "for tweet in tweetCrawler:\n",
        "  tweetList.append(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pD0VfpYmlne"
      },
      "outputs": [],
      "source": [
        "saveTweets(covidTweets, covid_set_idx, __location__ + location_covid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hYs36FPhz1m",
        "outputId": "a8b489b2-efe6-4c1a-b641-2c078ad9901d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 35.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "# install necessary lib\n",
        "!pip install torch torchvision transformers\n",
        "# restart now！！！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CebWLWb_h7MN",
        "outputId": "e79bc0f2-a028-4fb6-8917-e7899085a20b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import gensim\n",
        "import torch\n",
        "import nltk\n",
        "from torch.utils.data import Dataset\n",
        "# load google colab drive\n",
        "from google.colab import drive\n",
        "# load pretrained bert base model\n",
        "from transformers import BertModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHaD60fevImT"
      },
      "outputs": [],
      "source": [
        "# The data read is a list of list of dict\n",
        "def dataLoad(location_data):\n",
        "  f = open(__location__ + location_data)\n",
        "  data = f.readline()\n",
        "  return json.loads(data)\n",
        "\n",
        "trainData = dataLoad(location_train)\n",
        "devData = dataLoad(location_dev)\n",
        "testData = dataLoad(location_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCzZ4ko59Cbz",
        "outputId": "1d0846c5-e401-4e27-9f5a-773a456d228e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1475\n"
          ]
        }
      ],
      "source": [
        "# The label is a list of 1/0\n",
        "def labelLoad(location_data):\n",
        "  f = open(__location__ + location_data)\n",
        "  return [ 0 if line.rstrip('\\n') == 'nonrumour' else 1 for line in f]\n",
        "  \n",
        "location_train_label = 'train.label.txt'\n",
        "location_dev_label = 'dev.label.txt'\n",
        "\n",
        "trainLabel = labelLoad(location_train_label)\n",
        "devLabel = labelLoad(location_dev_label)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "nlpKaggleTask1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}